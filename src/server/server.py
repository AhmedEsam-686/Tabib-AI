import asyncio
import logging
import os
import uuid
import sys
from typing import AsyncGenerator, List, Dict

# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø§ÙƒÙŠØ¬Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙˆØª
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

import grpc
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams

import rag_pb2 as pb2
import rag_pb2_grpc as pb2_grpc
from src.core.config import model_config, server_config, PROJECT_ROOT

# ==============================================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙØ± ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
# ==============================================================================
# Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ÙƒÙˆÙ†ÙÙ‚ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ù„Ù„Ù€ vllm
MODEL_PATH = str(PROJECT_ROOT / model_config.model_path)
GRPC_PORT = server_config.port

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logging)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [SERVER] %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# 1. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ (The Brain: vLLM Engine)
# ==============================================================================
class IntelligentEngine:
    """
    Ù…Ø­Ø±Ùƒ vLLM Ù…Ø¯Ù…Ø¬ ÙˆÙ…Ø­Ø³Ù† Ù„Ù†Ù…Ø§Ø°Ø¬ Qwen Ù…Ø¹ Ø®Ø§ØµÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ±.
    (ØªÙ… Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ùƒ)
    """
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.engine = None
        self.tokenizer = None
        self.think_start_token = "<think>"
        self.think_end_token = "</think>"

    async def initialize(self):
        logger.info(f"ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.model_path}")
        logger.info("âš™ï¸  ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ³Ø±ÙŠØ¹ vLLM Ù…Ø¹ FP8...")
        
        engine_args = AsyncEngineArgs(
            model=self.model_path,
            quantization="fp8",
            max_model_len=8192,
            gpu_memory_utilization=0.90,
            tensor_parallel_size=1,
            enforce_eager=False,
            trust_remote_code=True,
            disable_log_stats=True
        )

        try:
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„.")
        except Exception as e:
            logger.critical(f"ğŸ”¥ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise

    async def generate_stream(self, messages: List[Dict[str, str]], request_id: str, **kwargs) -> AsyncGenerator[str, None]:
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 4096),
            top_p=kwargs.get("top_p", 0.8),
            # repetition_penalty=1.1, 
        )

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        results_generator = self.engine.generate(prompt, sampling_params, request_id)

        previous_text = ""
        async for request_output in results_generator:
            output = request_output.outputs[0]
            current_text = output.text
            new_chunk = current_text[len(previous_text):]
            previous_text = current_text
            if new_chunk:
                yield new_chunk

# ==============================================================================
# 2. Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø·Ù„Ø¨Ø§Øª (The Handler: gRPC Logic)
# ==============================================================================
class MedicalChatHandler(pb2_grpc.MedicalChatServiceServicer):
    def __init__(self, engine: IntelligentEngine):
        self.engine = engine

    async def GenerateStream(self, request, context):
        request_id = request.session_id if request.session_id else str(uuid.uuid4())
        chat_history = [{"role": msg.role, "content": msg.content} for msg in request.messages]
            
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        from src.core.config import generation_config
        
        gen_kwargs = {
            "max_tokens": request.config.max_tokens if request.config.max_tokens > 0 else generation_config.max_tokens,
            "temperature": request.config.temperature if request.config.temperature > 0 else generation_config.temperature,
            "top_p": request.config.top_p if request.config.top_p > 0 else generation_config.top_p,
        }

        logger.info(f"ğŸ“© Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯ [{request_id}] - {len(chat_history)} Ø±Ø³Ø§Ù„Ø©")

        try:
            async for token in self.engine.generate_stream(chat_history, request_id, **gen_kwargs):
                yield pb2.ChatResponse(token=token, is_finished=False)
            yield pb2.ChatResponse(is_finished=True)

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            await context.abort(grpc.StatusCode.INTERNAL, str(e))

# ==============================================================================
# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
# ==============================================================================
async def serve():
    logger.info("--- Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø³ÙŠØ±ÙØ± RAG Ø§Ù„Ø·Ø¨ÙŠ (Refactored) ---")
    engine = IntelligentEngine(model_path=MODEL_PATH)
    await engine.initialize()

    server = grpc.aio.server()
    pb2_grpc.add_MedicalChatServiceServicer_to_server(MedicalChatHandler(engine), server)
    
    listen_addr = f'[::]:{GRPC_PORT}'
    server.add_insecure_port(listen_addr)
    
    logger.info(f"ğŸ§ Ø§Ù„Ø³ÙŠØ±ÙØ± ÙŠØ³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ°: {listen_addr}")
    await server.start()
    await server.wait_for_termination()

if __name__ == "__main__":
    try:
        asyncio.run(serve())
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø³ÙŠØ±ÙØ±.")
