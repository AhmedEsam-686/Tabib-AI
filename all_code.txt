# Project Code Flattened Output
# Root: /home/ahmed/Desktop/TABIB-AI

================================================================================
FILE: mobile_app/ios/Flutter/ephemeral/flutter_lldb_helper.py
================================================================================
#
# Generated file, do not edit.
#

import lldb

def handle_new_rx_page(frame: lldb.SBFrame, bp_loc, extra_args, intern_dict):
    """Intercept NOTIFY_DEBUGGER_ABOUT_RX_PAGES and touch the pages."""
    base = frame.register["x0"].GetValueAsAddress()
    page_len = frame.register["x1"].GetValueAsUnsigned()

    # Note: NOTIFY_DEBUGGER_ABOUT_RX_PAGES will check contents of the
    # first page to see if handled it correctly. This makes diagnosing
    # misconfiguration (e.g. missing breakpoint) easier.
    data = bytearray(page_len)
    data[0:8] = b'IHELPED!'

    error = lldb.SBError()
    frame.GetThread().GetProcess().WriteMemory(base, data, error)
    if not error.Success():
        print(f'Failed to write into {base}[+{page_len}]', error)
        return

def __lldb_init_module(debugger: lldb.SBDebugger, _):
    target = debugger.GetDummyTarget()
    # Caveat: must use BreakpointCreateByRegEx here and not
    # BreakpointCreateByName. For some reasons callback function does not
    # get carried over from dummy target for the later.
    bp = target.BreakpointCreateByRegex("^NOTIFY_DEBUGGER_ABOUT_RX_PAGES$")
    bp.SetScriptCallbackFunction('{}.handle_new_rx_page'.format(__name__))
    bp.SetAutoContinue(True)
    print("-- LLDB integration loaded --")


================================================================================
FILE: rag_pb2.py
================================================================================
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: rag.proto
# Protobuf Python Version: 6.31.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    6,
    31,
    1,
    '',
    'rag.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\trag.proto\x12\x0bmedical_rag\"x\n\x0b\x43hatRequest\x12\x12\n\nsession_id\x18\x01 \x01(\t\x12&\n\x08messages\x18\x02 \x03(\x0b\x32\x14.medical_rag.Message\x12-\n\x06\x63onfig\x18\x03 \x01(\x0b\x32\x1d.medical_rag.GenerationConfig\"(\n\x07Message\x12\x0c\n\x04role\x18\x01 \x01(\t\x12\x0f\n\x07\x63ontent\x18\x02 \x01(\t\"J\n\x10GenerationConfig\x12\x12\n\nmax_tokens\x18\x01 \x01(\x05\x12\x13\n\x0btemperature\x18\x02 \x01(\x02\x12\r\n\x05top_p\x18\x03 \x01(\x02\"2\n\x0c\x43hatResponse\x12\r\n\x05token\x18\x01 \x01(\t\x12\x13\n\x0bis_finished\x18\x02 \x01(\x08\x32_\n\x12MedicalChatService\x12I\n\x0eGenerateStream\x12\x18.medical_rag.ChatRequest\x1a\x19.medical_rag.ChatResponse\"\x00\x30\x01\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'rag_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_CHATREQUEST']._serialized_start=26
  _globals['_CHATREQUEST']._serialized_end=146
  _globals['_MESSAGE']._serialized_start=148
  _globals['_MESSAGE']._serialized_end=188
  _globals['_GENERATIONCONFIG']._serialized_start=190
  _globals['_GENERATIONCONFIG']._serialized_end=264
  _globals['_CHATRESPONSE']._serialized_start=266
  _globals['_CHATRESPONSE']._serialized_end=316
  _globals['_MEDICALCHATSERVICE']._serialized_start=318
  _globals['_MEDICALCHATSERVICE']._serialized_end=413
# @@protoc_insertion_point(module_scope)


================================================================================
FILE: rag_pb2_grpc.py
================================================================================
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc
import warnings

import rag_pb2 as rag__pb2

GRPC_GENERATED_VERSION = '1.76.0'
GRPC_VERSION = grpc.__version__
_version_not_supported = False

try:
    from grpc._utilities import first_version_is_lower
    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
except ImportError:
    _version_not_supported = True

if _version_not_supported:
    raise RuntimeError(
        f'The grpc package installed is at version {GRPC_VERSION},'
        + ' but the generated code in rag_pb2_grpc.py depends on'
        + f' grpcio>={GRPC_GENERATED_VERSION}.'
        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
    )


class MedicalChatServiceStub(object):
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ø´Ø§Øª Ø§Ù„Ø·Ø¨ÙŠ
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.GenerateStream = channel.unary_stream(
                '/medical_rag.MedicalChatService/GenerateStream',
                request_serializer=rag__pb2.ChatRequest.SerializeToString,
                response_deserializer=rag__pb2.ChatResponse.FromString,
                _registered_method=True)


class MedicalChatServiceServicer(object):
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ø´Ø§Øª Ø§Ù„Ø·Ø¨ÙŠ
    """

    def GenerateStream(self, request, context):
        """Ø¯Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ØªÙ‚Ø¨Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØªØ¹ÙŠØ¯ Ø±Ø¯Ø§Ù‹ Ù…ØªØ¯ÙÙ‚Ø§Ù‹ (Streaming)
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_MedicalChatServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'GenerateStream': grpc.unary_stream_rpc_method_handler(
                    servicer.GenerateStream,
                    request_deserializer=rag__pb2.ChatRequest.FromString,
                    response_serializer=rag__pb2.ChatResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'medical_rag.MedicalChatService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))
    server.add_registered_method_handlers('medical_rag.MedicalChatService', rpc_method_handlers)


 # This class is part of an EXPERIMENTAL API.
class MedicalChatService(object):
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ø´Ø§Øª Ø§Ù„Ø·Ø¨ÙŠ
    """

    @staticmethod
    def GenerateStream(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_stream(
            request,
            target,
            '/medical_rag.MedicalChatService/GenerateStream',
            rag__pb2.ChatRequest.SerializeToString,
            rag__pb2.ChatResponse.FromString,
            options,
            channel_credentials,
            insecure,
            call_credentials,
            compression,
            wait_for_ready,
            timeout,
            metadata,
            _registered_method=True)


================================================================================
FILE: run_app.py
================================================================================
import os
import sys
import subprocess

def main():
    """
    Ø³ÙƒØ±ÙŠØ¨Øª Ø¨Ø³ÙŠØ· Ù„ØªØ´ØºÙŠÙ„ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.
    """
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©)...")
    
    # Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    app_path = "src/ui/main.py"
    
    if not os.path.exists(app_path):
        print(f"âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù {app_path}")
        return

    # ØªØ´ØºÙŠÙ„ Streamlit
    try:
        subprocess.run(["streamlit", "run", app_path], check=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ÙˆØ¯Ø§Ø¹Ø§Ù‹!")

if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/flatten_project.py
================================================================================
#!/usr/bin/env python3
"""
Flatten all code files in the project into a single text file.

Usage:
  python scripts/flatten_project.py --output all_code.txt
"""

from __future__ import annotations

import argparse
import os
from pathlib import Path
from typing import Iterable, List, Set, Tuple


DEFAULT_EXTENSIONS = {
    ".py",
}

DEFAULT_EXCLUDE_DIRS = {
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    "node_modules",
    "build",
    "dist",
    "chroma_db_storage",
    "dataset",
    "assets",
    ".idea",
    ".vscode",
}

DEFAULT_MAX_FILE_BYTES = 1_000_000  # 1 MB


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Flatten project code into a single text file."
    )
    parser.add_argument(
        "--root",
        type=str,
        default=None,
        help="Project root directory (default: repo root).",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="project_code.txt",
        help="Output file path (default: project_code.txt).",
    )
    parser.add_argument(
        "--include-ext",
        type=str,
        default=",".join(sorted(DEFAULT_EXTENSIONS)),
        help="Comma-separated list of file extensions to include.",
    )
    parser.add_argument(
        "--exclude-dir",
        type=str,
        default=",".join(sorted(DEFAULT_EXCLUDE_DIRS)),
        help="Comma-separated list of directory names to exclude.",
    )
    parser.add_argument(
        "--max-bytes",
        type=int,
        default=DEFAULT_MAX_FILE_BYTES,
        help="Skip files larger than this size (bytes).",
    )
    return parser.parse_args()


def is_binary_file(path: Path) -> bool:
    try:
        with path.open("rb") as f:
            chunk = f.read(2048)
        if b"\x00" in chunk:
            return True
        # Heuristic: if the chunk can't be decoded as UTF-8, treat as binary.
        try:
            chunk.decode("utf-8")
        except UnicodeDecodeError:
            return True
        return False
    except OSError:
        return True


def iter_files(
    root: Path, include_exts: Set[str], exclude_dirs: Set[str]
) -> Iterable[Path]:
    for dirpath, dirnames, filenames in os.walk(root):
        # Prune excluded directories in-place
        dirnames[:] = [d for d in dirnames if d not in exclude_dirs]
        for filename in filenames:
            path = Path(dirpath) / filename
            if path.suffix.lower() in include_exts:
                yield path


def normalize_extensions(exts: str) -> Set[str]:
    result: Set[str] = set()
    for item in exts.split(","):
        item = item.strip()
        if not item:
            continue
        if not item.startswith("."):
            item = "." + item
        result.add(item.lower())
    return result


def normalize_dirnames(items: str) -> Set[str]:
    return {item.strip() for item in items.split(",") if item.strip()}


def read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="replace")


def flatten_project(
    root: Path,
    output: Path,
    include_exts: Set[str],
    exclude_dirs: Set[str],
    max_bytes: int,
) -> Tuple[int, List[Path]]:
    files = sorted(iter_files(root, include_exts, exclude_dirs))
    skipped: List[Path] = []
    written_count = 0

    header = [
        "# Project Code Flattened Output",
        f"# Root: {root}",
        "",
    ]

    output.parent.mkdir(parents=True, exist_ok=True)
    with output.open("w", encoding="utf-8") as out:
        out.write("\n".join(header))

        for path in files:
            try:
                if path.stat().st_size > max_bytes:
                    skipped.append(path)
                    continue
                if is_binary_file(path):
                    skipped.append(path)
                    continue
                rel = path.relative_to(root)
                out.write("\n")
                out.write("=" * 80 + "\n")
                out.write(f"FILE: {rel}\n")
                out.write("=" * 80 + "\n")
                out.write(read_text(path))
                out.write("\n")
                written_count += 1
            except OSError:
                skipped.append(path)
                continue

    return written_count, skipped


def main() -> int:
    args = parse_args()
    script_dir = Path(__file__).resolve().parent
    repo_root = script_dir.parent
    root = Path(args.root).resolve() if args.root else repo_root
    output = Path(args.output).resolve()
    include_exts = normalize_extensions(args.include_ext)
    exclude_dirs = normalize_dirnames(args.exclude_dir)

    written_count, skipped = flatten_project(
        root=root,
        output=output,
        include_exts=include_exts,
        exclude_dirs=exclude_dirs,
        max_bytes=args.max_bytes,
    )

    print(f"Wrote {written_count} files to {output}")
    if skipped:
        print(f"Skipped {len(skipped)} files (binary/large/unreadable).")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


================================================================================
FILE: src/core/client.py
================================================================================
import sys
import os

# ==========================================================================
# ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ù„ÙŠ Ù„Ù…Ù†Ø¹ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ Ø¯ÙˆÙ† Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
# ==========================================================================
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'

import grpc
import chromadb
from sentence_transformers import SentenceTransformer
import streamlit as st
import asyncio
from typing import List, Dict, Generator, Optional

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø°Ø±ÙŠ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø±ÙˆØªÙˆ ÙÙŠ Ø§Ù„Ø±ÙˆØª
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

import rag_pb2 as pb2
import rag_pb2_grpc as pb2_grpc
from src.core.config import db_config, model_config, server_config, generation_config

class MedicalClient:
    def __init__(self):
        self.collection = None
        self.embed_model = None
        self._load_resources()

    def _load_resources(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ (Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª + Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†)"""
        try:
            # 1. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ChromaDB
            chroma_client = chromadb.PersistentClient(path=db_config.db_path)
            self.collection = chroma_client.get_collection(name=db_config.collection_name)
            
            # 2. ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† (ØµØºÙŠØ± ÙˆØ³Ø±ÙŠØ¹ Ø¹Ù„Ù‰ CPU)
            self.embed_model = SentenceTransformer(model_config.embedding_model, device=model_config.device)
            
        except Exception as e:
            st.error(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {e}")

    def retrieve_documents(self, query: str, n_results: int = 4) -> List[Dict[str, str]]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        if not self.collection or not self.embed_model:
            return []
        
        try:
            query_vec = self.embed_model.encode([query]).tolist()
            results = self.collection.query(
                query_embeddings=query_vec,
                n_results=n_results
            )
            
            documents = []
            if results['documents']:
                for i, doc in enumerate(results['documents'][0]):
                    meta = results['metadatas'][0][i]
                    # Calculate simple confidence from distance (smaller is better, usually range 0-2 for cosine/l2)
                    # This is a heuristic approximation
                    distance = results['distances'][0][i] if 'distances' in results and results['distances'] else 0.5
                    confidence = max(0, min(100, int((1 - distance) * 100))) if distance < 1.0 else int(100/(distance+1))
                    
                    documents.append({
                        "question": meta.get('original_question', 'Ø³Ø¤Ø§Ù„ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'),
                        "answer": meta.get('answer', doc),
                        "confidence": confidence
                    })
            return documents
        except Exception as e:
            st.warning(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []

    async def generate_response(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        """Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø³ÙŠØ±ÙØ± Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        try:
            async with grpc.aio.insecure_channel(server_config.address) as channel:
                stub = pb2_grpc.MedicalChatServiceStub(channel)
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
                proto_messages = [
                    pb2.Message(role=m['role'], content=m['content']) 
                    for m in messages
                ]
                
                request = pb2.ChatRequest(
                    session_id="ui-client",
                    messages=proto_messages,
                    config=pb2.GenerationConfig(
                        max_tokens=generation_config.max_tokens,
                        temperature=generation_config.temperature,
                        top_p=generation_config.top_p
                    )
                )
                
                async for response in stub.GenerateStream(request):
                    yield response.token
                    
        except grpc.RpcError as e:
            yield f"\n[Ø®Ø·Ø£ Ø§ØªØµØ§Ù„]: {e.details()}"
        except Exception as e:
            yield f"\n[Ø®Ø·Ø£]: {e}"

# Singleton instance
# Ù†Ø³ØªØ®Ø¯Ù… ØªÙƒÙ†ÙŠÙƒ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ø¹Ø¯Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ ÙƒÙ„ ØªØ­Ø¯ÙŠØ« Ù„Ù„ØµÙØ­Ø©
@st.cache_resource
def get_client() -> MedicalClient:
    return MedicalClient()


================================================================================
FILE: src/core/config.py
================================================================================
import os
from pathlib import Path
from dataclasses import dataclass

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø°Ø±ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ (Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ src)
PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()

@dataclass
class AppConfig:
    page_title: str = "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ"
    page_icon: str = "ğŸ©º"
    layout: str = "wide"
    initial_sidebar_state: str = "expanded"

@dataclass
class ModelConfig:
    # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø¬Ø°Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    model_path: str = "Qwen3-4B-Thinking-2507-FP8"
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø¥Ù†ØªØ±Ù†Øª
    embedding_model: str = "/home/ahmed/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d"
    device: str = "cpu"  # Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ· ÙÙŠ Ø§Ù„ÙƒÙ„Ø§ÙŠÙ†Øª

@dataclass
class ServerConfig:
    host: str = "localhost"
    port: str = "50052"
    
    @property
    def address(self) -> str:
        return f"{self.host}:{self.port}"

@dataclass
class DatabaseConfig:
    # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    db_relative_path: str = "chroma_db_storage"
    collection_name: str = "medical_knowledge_base"
    
    @property
    def db_path(self) -> str:
        return str(PROJECT_ROOT / self.db_relative_path)

@dataclass
class GenerationConfigDefaults:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    max_tokens: int = 8192
    temperature: float = 0.7
    top_p: float = 0.8

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
app_config = AppConfig()
model_config = ModelConfig()
server_config = ServerConfig()
db_config = DatabaseConfig()
generation_config = GenerationConfigDefaults()

# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
def validate_paths():
    if not os.path.exists(db_config.db_path):
        print(f"Warning: Database path does not exist at {db_config.db_path}")
    
    model_full_path = PROJECT_ROOT / model_config.model_path
    if not os.path.exists(model_full_path):
        print(f"Warning: Model path does not exist at {model_full_path}")


================================================================================
FILE: src/core/prompts.py
================================================================================
# ==============================================================================
# ğŸ§  Ù…Ù„Ù Ø§Ù„ØªÙ„Ù‚ÙŠÙ†Ø§Øª (Prompts)
# ==============================================================================
# ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ÙØµÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø®Ø§ØµØ© Ø¨ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ (Clean Code).
# ==============================================================================

# Ø§Ù„ØªÙ„Ù‚ÙŠÙ†Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù†Ø¸Ø§Ù… (System Message)
MEDICAL_AGENT_SYSTEM_PROMPT = """
Ø£Ù†Øª "Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø·Ø¨ÙŠØ©"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ®ØµØµ ÙˆØ¢Ù…Ù†.

### Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹ ÙˆÙÙ‚Ø·** Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù‡Ø§ ÙÙŠ Ù‚Ø³Ù… "Ø§Ù„Ø³ÙŠØ§Ù‚" (Context).

### Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØµØ§Ø±Ù…Ø© (Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ Ø§Ù„Ø¹Ù…Ù„):
1. **Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„ÙˆØ­ÙŠØ¯ Ù„Ù„Ø­Ù‚ÙŠÙ‚Ø©:** ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠ "Ø§Ù„Ø³ÙŠØ§Ù‚" ÙÙ‚Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©. ÙŠÙ…Ù†Ø¹ Ù…Ù†Ø¹Ø§Ù‹ Ø¨Ø§ØªØ§Ù‹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚.
2. **Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:** Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø£Ùˆ Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù†Ù‡Ø§) Ø¯Ø§Ø®Ù„ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ø±ÙÙ‚ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø¯ Ø¨ÙˆØ¶ÙˆØ­ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©: "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªØªÙˆÙØ± Ù„Ø¯ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©." Ù„Ø§ ØªØ­Ø§ÙˆÙ„ ØªØ£Ù„ÙŠÙ Ø£Ùˆ ØªØ®Ù…ÙŠÙ† Ø¥Ø¬Ø§Ø¨Ø©.
3. **Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„ØªØ®ØµØµ:** Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠ (Ù…Ø«Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŒ Ø§Ù„Ø³ÙŠØ§Ø³Ø©ØŒ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŒ Ø§Ù„Ø·Ù‚Ø³)ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±ÙØ¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù„Ø¨Ø§Ù‚Ø© ÙˆØªÙˆØ¶Ø­ Ø£Ù†Ùƒ Ù†Ø¸Ø§Ù… Ù…Ø®ØµØµ Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© ÙÙ‚Ø·.
4. **Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„:** Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø£Ùˆ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙ…ØªØ±Ø§Ø¨Ø·Ø©.
5. **Ø¥Ø®Ù„Ø§Ø¡ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©:** ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ©ØŒ Ø£Ø¶Ù ØªÙ†Ø¨ÙŠÙ‡Ø§Ù‹ Ø¨Ø£Ù†Ùƒ Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø§ ØªØºÙ†ÙŠ Ø¹Ù† Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø®ØªØµ.

### Ø§Ù„Ù†Ø¨Ø±Ø© ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨:
- Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ØŒ ÙˆØ§Ø¶Ø­Ø©ØŒ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ ÙˆÙ…Ù‡Ù†ÙŠØ©.
- Ø£Ø³Ù„ÙˆØ¨ Ù…ØªØ¹Ø§Ø·Ù ÙˆÙ„ÙƒÙ† Ù…ÙˆØ¶ÙˆØ¹ÙŠ.
- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ù†Ù‚Ø§Ø· Ø£Ùˆ ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©.
""".strip()

# Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (User Prompt Template)
def format_rag_prompt(user_query: str, context_str: str) -> str:
    """ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    return f"""
ğŸ“š **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ«Ù‚:**
{context_str}

ğŸ“ **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶:**
{user_query}

ğŸ› ï¸ **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
- Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡.
- Ø§Ø´Ø±Ø­ Ø¨Ø¨Ø³Ø§Ø·Ø© ÙˆÙˆØ¶ÙˆØ­.
"""


================================================================================
FILE: src/server/mock_server.py
================================================================================
import asyncio
import logging
import os
import sys
import time
from typing import AsyncGenerator

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø±ÙˆØªÙˆ
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

import grpc
import rag_pb2 as pb2
import rag_pb2_grpc as pb2_grpc

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO, format='%(asctime)s [MOCK SERVER] %(message)s')
logger = logging.getLogger(__name__)

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
PORT = 50052

class MockMedicalChatHandler(pb2_grpc.MedicalChatServiceServicer):
    """
    Ù…Ø­Ø§ÙƒÙŠ Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø´Ø§Øª Ø§Ù„Ø·Ø¨ÙŠ.
    """
    async def GenerateStream(self, request, context):
        """
        ÙŠØ³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨ ÙˆÙŠØ±Ø¯ Ø¨Ù†Øµ Ø«Ø§Ø¨Øª ÙˆÙ…Ø­Ø§ÙƒÙŠ Ù„Ù„ØªØ¯ÙÙ‚ (Streaming).
        """
        request_id = request.session_id if request.session_id else "unknown_session"
        user_message = request.messages[-1].content if request.messages else ""
        
        logger.info(f"ğŸ“© Ø§Ø³ØªÙ„Ø§Ù… Ø·Ù„Ø¨ ÙˆÙ‡Ù…ÙŠ [{request_id}] - Ø§Ù„Ø±Ø³Ø§Ù„Ø©: {user_message[:30]}...")

        # Ù†Øµ Ø§Ù„Ø±Ø¯ Ø§Ù„ÙˆÙ‡Ù…

================================================================================
FILE: src/server/server.py
================================================================================
import asyncio
import logging
import os
import uuid
import sys
from typing import AsyncGenerator, List, Dict

# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¤ÙŠØ© Ø§Ù„Ø¨Ø§ÙƒÙŠØ¬Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙˆØª
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

import grpc
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams

import rag_pb2 as pb2
import rag_pb2_grpc as pb2_grpc
from src.core.config import model_config, server_config, PROJECT_ROOT

# ==============================================================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙØ± ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
# ==============================================================================
# Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ÙƒÙˆÙ†ÙÙ‚ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ù„Ù„Ù€ vllm
MODEL_PATH = str(PROJECT_ROOT / model_config.model_path)
GRPC_PORT = server_config.port

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logging)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [SERVER] %(message)s')
logger = logging.getLogger(__name__)

# ==============================================================================
# 1. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ (The Brain: vLLM Engine)
# ==============================================================================
class IntelligentEngine:
    """
    Ù…Ø­Ø±Ùƒ vLLM Ù…Ø¯Ù…Ø¬ ÙˆÙ…Ø­Ø³Ù† Ù„Ù†Ù…Ø§Ø°Ø¬ Qwen Ù…Ø¹ Ø®Ø§ØµÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ±.
    (ØªÙ… Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ùƒ)
    """
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.engine = None
        self.tokenizer = None
        self.think_start_token = "<think>"
        self.think_end_token = "</think>"

    async def initialize(self):
        logger.info(f"ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.model_path}")
        logger.info("âš™ï¸  ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ³Ø±ÙŠØ¹ vLLM Ù…Ø¹ FP8...")
        
        engine_args = AsyncEngineArgs(
            model=self.model_path,
            quantization="fp8",
            max_model_len=8192,
            gpu_memory_utilization=0.90,
            tensor_parallel_size=1,
            enforce_eager=False,
            trust_remote_code=True,
            disable_log_stats=True
        )

        try:
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„.")
        except Exception as e:
            logger.critical(f"ğŸ”¥ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise

    async def generate_stream(self, messages: List[Dict[str, str]], request_id: str, **kwargs) -> AsyncGenerator[str, None]:
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 4096),
            top_p=kwargs.get("top_p", 0.8),
            # repetition_penalty=1.1, 
        )

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        results_generator = self.engine.generate(prompt, sampling_params, request_id)

        previous_text = ""
        async for request_output in results_generator:
            output = request_output.outputs[0]
            current_text = output.text
            new_chunk = current_text[len(previous_text):]
            previous_text = current_text
            if new_chunk:
                yield new_chunk

# ==============================================================================
# 2. Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø·Ù„Ø¨Ø§Øª (The Handler: gRPC Logic)
# ==============================================================================
class MedicalChatHandler(pb2_grpc.MedicalChatServiceServicer):
    def __init__(self, engine: IntelligentEngine):
        self.engine = engine

    async def GenerateStream(self, request, context):
        request_id = request.session_id if request.session_id else str(uuid.uuid4())
        chat_history = [{"role": msg.role, "content": msg.content} for msg in request.messages]
            
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        from src.core.config import generation_config
        
        gen_kwargs = {
            "max_tokens": request.config.max_tokens if request.config.max_tokens > 0 else generation_config.max_tokens,
            "temperature": request.config.temperature if request.config.temperature > 0 else generation_config.temperature,
            "top_p": request.config.top_p if request.config.top_p > 0 else generation_config.top_p,
        }

        logger.info(f"ğŸ“© Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯ [{request_id}] - {len(chat_history)} Ø±Ø³Ø§Ù„Ø©")

        try:
            async for token in self.engine.generate_stream(chat_history, request_id, **gen_kwargs):
                yield pb2.ChatResponse(token=token, is_finished=False)
            yield pb2.ChatResponse(is_finished=True)

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            await context.abort(grpc.StatusCode.INTERNAL, str(e))

# ==============================================================================
# 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
# ==============================================================================
async def serve():
    logger.info("--- Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø³ÙŠØ±ÙØ± RAG Ø§Ù„Ø·Ø¨ÙŠ (Refactored) ---")
    engine = IntelligentEngine(model_path=MODEL_PATH)
    await engine.initialize()

    server = grpc.aio.server()
    pb2_grpc.add_MedicalChatServiceServicer_to_server(MedicalChatHandler(engine), server)
    
    listen_addr = f'[::]:{GRPC_PORT}'
    server.add_insecure_port(listen_addr)
    
    logger.info(f"ğŸ§ Ø§Ù„Ø³ÙŠØ±ÙØ± ÙŠØ³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ°: {listen_addr}")
    await server.start()
    await server.wait_for_termination()

if __name__ == "__main__":
    try:
        asyncio.run(serve())
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø³ÙŠØ±ÙØ±.")


================================================================================
FILE: src/ui/main.py
================================================================================
import streamlit as st
import asyncio
import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.core.config import app_config
from src.core.client import get_client
import src.ui.components as components

# ==========================================================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
# ==========================================================================
st.set_page_config(
    page_title=app_config.page_title,
    page_icon=app_config.page_icon,
    layout=app_config.layout,
    initial_sidebar_state=app_config.initial_sidebar_state
)

# ==========================================================================
# 2. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø«ÙŠÙ…Ø§Øª (Themes Management)
# ==========================================================================
def inject_theme():
    """Ø­Ù‚Ù† Ù…ØªØºÙŠØ±Ø§Øª CSS Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    if "theme" not in st.session_state:
        st.session_state.theme = "dark" # Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¯Ø§ÙƒÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…

    # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù„ÙƒÙ„ ÙˆØ¶Ø¹
    themes = {
        "light": """
            :root {
                --bg-main: #e2e8f0;         /* Slate 200 - Ø±Ù…Ø§Ø¯ÙŠ ÙØ¶ÙŠ Ù…Ø±ÙŠØ­ (Ø¨Ø¯Ù„ Ø§Ù„Ø£Ø¨ÙŠØ¶ Ø§Ù„Ø³Ø§Ø·Ø¹) */
                --bg-sidebar: #f8fafc;      /* Slate 50 - ÙØ§ØªØ­ Ø¬Ø¯Ø§Ù‹ */
                --text-primary: #1e293b;    /* Slate 800 - Ø¯Ø§ÙƒÙ† Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© */
                --text-secondary: #475569;  /* Slate 600 */
                --card-bg: #ffffff;         /* Ø£Ø¨ÙŠØ¶ Ù†Ù‚ÙŠ Ù„Ù„Ø¨Ø±ÙˆØ² */
                --border-color: #cbd5e1;    /* Slate 300 */
                --primary-color: #0891b2;   /* Cyan 600 */
                --secondary-color: #0e7490; /* Cyan 700 */
                --msg-user-bg: #cffafe;     /* Cyan 100 */
                --msg-user-border: #a5f3fc; /* Cyan 200 */
                --msg-bot-bg: #ffffff;
            }
        """,
        "dark": """
            :root {
                --bg-main: #0f172a;        /* Slate 900 */
                --bg-sidebar: #1e293b;      /* Slate 800 */
                --text-primary: #f1f5f9;    /* Slate 100 */
                --text-secondary: #94a3b8;  /* Slate 400 */
                --card-bg: #1e293b;         /* Slate 800 */
                --border-color: #334155;    /* Slate 700 */
                --primary-color: #38bdf8;   /* Sky 400 */
                --secondary-color: #7dd3fc; /* Sky 300 */
                --msg-user-bg: #1e293b;     /* Slate 800 */
                --msg-user-border: #334155; /* Slate 700 */
                --msg-bot-bg: #0f172a;      /* Slate 900 */
            }
        """
    }

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³ØªØ§ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    css_path = os.path.join(os.path.dirname(__file__), "styles.css")
    with open(css_path, "r") as f:
        base_css = f.read()

    # Ø¯Ù…Ø¬ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù…Ø¹ Ø§Ù„Ø³ØªØ§ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    chosen_theme = themes[st.session_state.theme]
    full_css = f"<style>{chosen_theme}\n{base_css}</style>"
    st.markdown(full_css, unsafe_allow_html=True)

inject_theme()

# ==========================================================================
# 3. Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ==========================================================================
def main():
    # --- Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ---
    with st.sidebar:
        st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        
        # Ø²Ø± ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ø«ÙŠÙ…
        theme_label = "ğŸŒ™ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù„ÙŠÙ„ÙŠ" if st.session_state.theme == "light" else "â˜€ï¸ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø±ÙŠ"
        if st.button(theme_label, use_container_width=True):
            st.session_state.theme = "dark" if st.session_state.theme == "light" else "light"
            st.rerun()
            
        st.divider()
        
        # Ø²Ø± Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
        if st.button("ğŸ—‘ï¸ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©", type="primary", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.divider()
        st.caption("v.2.1 | Medical AI Assistant")

    # --- Ø§Ù„ØªØ±ÙˆÙŠØ³Ø© ---
    components.render_header()

    # --- ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ---
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ğŸ©º. Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„ØªØ´Ø®ÙŠØµØ§ØªØŒ Ø§Ù„Ø£Ø¯ÙˆÙŠØ©ØŒ Ø£Ùˆ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶."}
        ]

    # --- Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ---
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ---
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø·Ø¨ÙŠ Ù‡Ù†Ø§..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            process_response(prompt)

def process_response(user_query: str):
    client = get_client()
    
    # 1. Ø§Ù„Ø¨Ø­Ø«
    with st.status("ğŸ” Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø·Ø¨ÙŠØ©...", expanded=False) as status:
        documents = client.retrieve_documents(user_query, n_results=4)
        if documents:
            status.update(label="âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹ Ù…ÙˆØ«ÙˆÙ‚Ø©", state="complete")
            st.write("---")
            for i, doc in enumerate(documents, 1):
                components.render_source_card(i, doc['question'], doc['answer'], doc.get('confidence', 0))
            context_str = "\n".join([f"- Ø³: {d['question']}\n  Ø¬: {d['answer']}" for d in documents])
        else:
            status.update(label="âš ï¸ ÙŠØªÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©", state="complete")
            context_str = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ØµØ§Ø¯Ø± Ù…Ø­Ø¯Ø¯Ø©."

    # 2. Ø§Ù„ØªØ¬Ù‡ÙŠØ²
    # 2. Ø§Ù„ØªØ¬Ù‡ÙŠØ²
    from src.core.prompts import MEDICAL_AGENT_SYSTEM_PROMPT, format_rag_prompt
    from src.core.config import generation_config
    
    rag_prompt = format_rag_prompt(user_query, context_str)
    
    current_messages = [
        {"role": "system", "content": MEDICAL_AGENT_SYSTEM_PROMPT},
        {"role": "user", "content": rag_prompt}
    ]

    # 3. Ø§Ù„Ø¨Ø«
    run_stream_ui(client, current_messages)

def run_stream_ui(client, messages):
    thought_expander = st.status("ğŸ§  Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø³Ø±ÙŠØ±ÙŠ...", expanded=True)
    thought_placeholder = thought_expander.empty()
    answer_placeholder = st.empty()
    
    full_text = ""
    thinking_text = ""
    answer_text = ""
    is_thinking_mode = True
    
    async def stream():
        nonlocal full_text, thinking_text, answer_text, is_thinking_mode
        async for chunk in client.generate_response(messages):
            full_text += chunk
            if is_thinking_mode:
                if "</think>" in full_text:
                    is_thinking_mode = False
                    parts = full_text.split("</think>")
                    thinking_text = parts[0].replace("<think>", "").strip()
                    thought_placeholder.markdown(thinking_text)
                    thought_expander.update(label="âœ… Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„", state="complete", expanded=False)
                    answer_text = parts[-1]
                    answer_placeholder.markdown(answer_text + "â–Œ")
                else:
                    display = full_text.replace("<think>", "")
                    thought_placeholder.markdown(display + "â–Œ")
            else:
                parts = full_text.split("</think>")
                answer_text = parts[-1]
                answer_placeholder.markdown(answer_text + "â–Œ")

    try:
        asyncio.run(stream())
        answer_placeholder.markdown(answer_text)
        if is_thinking_mode: 
             thought_expander.update(label="Ø§ÙƒØªÙ…Ù„", state="complete")
             answer_placeholder.markdown(full_text.replace("<think>", ""))
             answer_text = full_text
        st.session_state.messages.append({"role": "assistant", "content": answer_text})
    except Exception as e:
        st.error(f"Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    main()

